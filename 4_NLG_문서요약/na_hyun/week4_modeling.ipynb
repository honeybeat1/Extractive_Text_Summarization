{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "mount_file_id": "1yemVddNPWCjjSOJ7mToUpcHMAYzkM6Jo",
      "authorship_tag": "ABX9TyMx0UZvmDMTyUUUYto1W/Dy"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# ! pip install konlpy"
      ],
      "metadata": {
        "id": "DlD2pY_u-gqA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zuciK6th8CGv"
      },
      "outputs": [],
      "source": [
        "# ! git clone https://github.com/seujung/KoBART-summarization.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/KoBART-summarization/data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_4c7N0-8NQ7",
        "outputId": "2b8210f7-783e-4d54-824e-8f3def7256c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoBART-summarization/data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os \n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "PqTS0krVOx6y"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# seed\n",
        "seed = 7777\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# device type\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"# available GPUs : {torch.cuda.device_count()}\")\n",
        "    print(f\"GPU name : {torch.cuda.get_device_name()}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Dc1kI1IO25Y",
        "outputId": "a660d812-905b-4022-c744-75b44cd761d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# available GPUs : 1\n",
            "GPU name : Tesla P100-PCIE-16GB\n",
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/sports_news_KoBART.csv')\n",
        "df = df[['TITLE', 'CONTENT', 'PUBLISH_DT', 'KoBART']]\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "CuIU_-RzNGjx",
        "outputId": "78bebf6e-09dc-4b61-a95b-ff8c3ff5bf4c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      TITLE  \\\n",
              "0              스털링 다이빙 논란 종결 오른쪽 다리 접촉 있었잖아   \n",
              "1             디 마리아 없다 유로X코파 베스트11 이탈리아만 7명   \n",
              "2              슈퍼컴퓨터 예측 맨시티 우승맨유 4위 토트넘은 6위   \n",
              "3     이재성 완벽한 프로 마인츠서 성공할 것 킬 디렉터의 애정 듬뿍 응원   \n",
              "4     홈킷과 딴판 바르사 팬들 NEW 어웨이 셔츠 호평 가장 좋아하는 색   \n",
              "...                                     ...   \n",
              "7325       이제 홈팬 야유 받는 먹튀 선수 차비 감독 조차 그만 해라   \n",
              "7326            오피셜 성남 만 17세 유스 김지수와 준프로 계약   \n",
              "7327       오베르마스 아약스서 쫓겨난다여성 동료들에게 부적절한 메시지   \n",
              "7328      바르사 차비 감독 트라오레 데뷔전 활약에 깜짝몸이 야수 같아   \n",
              "7329    데파이팔카오산체스디 마리아 퍼거슨 경 은퇴 후 맨유 워스트 11   \n",
              "\n",
              "                                                CONTENT  PUBLISH_DT  \\\n",
              "0     유럽축구연맹 유로 2020 심판위원장 로베르토 로세티가 잉글랜드와 덴마크전에 나온 ...  2021-07-15   \n",
              "1     지난달 시작된 유로와 코파 아메리카가 11일을 끝으로 막을 내렸다 이탈리아는 결승전...  2021-07-15   \n",
              "2     새 시즌이 시작하기도 전에 슈퍼컴퓨터가 예상한 순위가 나왔다 영국 매체 스포츠 바이...  2021-07-15   \n",
              "3     홀슈타인 킬 우베 스토버 디렉터가 이재성을 향해 응원 메시지를 띄웠다 이재성은 20...  2021-07-15   \n",
              "4     FC 바르셀로나가 새 시즌 원정 유니폼을 공개했다 팬들은 만족스럽다는 반응이다 바르...  2021-07-15   \n",
              "...                                                 ...         ...   \n",
              "7325  FC바르셀로나 팬들에게 우스망 뎀벨레는 밉상이 되어버렸다 바르사는 7일 오전 0시 ...  2022-02-07   \n",
              "7326  성남FC가 만17세 2004년생 수비수 김지수와 준프로 계약을 체결했다 김지수는 1...  2022-02-07   \n",
              "7327  레전드 마르크 오베르마스가 아약스에서 쫓겨났다 이유는 굉장히 굴욕적이었다 아약스는 ...  2022-02-07   \n",
              "7328  FC 바르셀로나 차비 에르난데스 감독이 데뷔전을 치른 아다마 트라오레를 극찬했다 바...  2022-02-07   \n",
              "7329  이름값만 화려한 맨유 워스트 11이 공개됐다 영국 매체 익스프레스는 7일 알렉스 퍼...  2022-02-07   \n",
              "\n",
              "                                                 KoBART  \n",
              "0     유럽축구연맹 유로 2020 심판위원장 로베르토 로세티가 잉글랜드와 덴마크전에 나온 ...  \n",
              "1              지난달 시작된 시작된 유로와 코파 아메리카가 11일을 끝으로 막을 내렸다  \n",
              "2     지난 시즌 초반 고초를 겪었던 맨체스터 시티가 승점 88을 쌓아 경쟁 팀들을 손쉽게...  \n",
              "3         홀슈타인 킬 우베 스토버 디렉터가 킬의 에이스 이재성을 향해 응원 메시지를 띄웠다  \n",
              "4     바르사는 15일 공식 홈페이지를 통해 20212022시즌에 입을 어웨이 킷을 선보였...  \n",
              "...                                                 ...  \n",
              "7325  7일 오전 0시 15분 스페인 바르셀로나 캄노우에서 열린 아틀레티코 마드리드와 20...  \n",
              "7326     성남FC가 만17세 2004년생 2004년생 수비수 김지수와 준프로 계약을 체결했다  \n",
              "7327  네덜란드 출신의 레전드 마르크 오베르마스가 아약스에서 쫓겨났다 이유는 굉장히 굴욕적이었다  \n",
              "7328  지난겨울 이적시장에서 울버햄프턴을 떠나 바르사로 임대 이적했고 이날 경기에 선발 출...  \n",
              "7329  영국 매체 익스프레스는 7일 알렉스 퍼거슨 경 은퇴 후 맨유의 워스트 11을 선정했...  \n",
              "\n",
              "[7330 rows x 4 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-32497e19-cb54-4d53-9b8a-caeca5dab297\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TITLE</th>\n",
              "      <th>CONTENT</th>\n",
              "      <th>PUBLISH_DT</th>\n",
              "      <th>KoBART</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>스털링 다이빙 논란 종결 오른쪽 다리 접촉 있었잖아</td>\n",
              "      <td>유럽축구연맹 유로 2020 심판위원장 로베르토 로세티가 잉글랜드와 덴마크전에 나온 ...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>유럽축구연맹 유로 2020 심판위원장 로베르토 로세티가 잉글랜드와 덴마크전에 나온 ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>디 마리아 없다 유로X코파 베스트11 이탈리아만 7명</td>\n",
              "      <td>지난달 시작된 유로와 코파 아메리카가 11일을 끝으로 막을 내렸다 이탈리아는 결승전...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>지난달 시작된 시작된 유로와 코파 아메리카가 11일을 끝으로 막을 내렸다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>슈퍼컴퓨터 예측 맨시티 우승맨유 4위 토트넘은 6위</td>\n",
              "      <td>새 시즌이 시작하기도 전에 슈퍼컴퓨터가 예상한 순위가 나왔다 영국 매체 스포츠 바이...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>지난 시즌 초반 고초를 겪었던 맨체스터 시티가 승점 88을 쌓아 경쟁 팀들을 손쉽게...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>이재성 완벽한 프로 마인츠서 성공할 것 킬 디렉터의 애정 듬뿍 응원</td>\n",
              "      <td>홀슈타인 킬 우베 스토버 디렉터가 이재성을 향해 응원 메시지를 띄웠다 이재성은 20...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>홀슈타인 킬 우베 스토버 디렉터가 킬의 에이스 이재성을 향해 응원 메시지를 띄웠다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홈킷과 딴판 바르사 팬들 NEW 어웨이 셔츠 호평 가장 좋아하는 색</td>\n",
              "      <td>FC 바르셀로나가 새 시즌 원정 유니폼을 공개했다 팬들은 만족스럽다는 반응이다 바르...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>바르사는 15일 공식 홈페이지를 통해 20212022시즌에 입을 어웨이 킷을 선보였...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7325</th>\n",
              "      <td>이제 홈팬 야유 받는 먹튀 선수 차비 감독 조차 그만 해라</td>\n",
              "      <td>FC바르셀로나 팬들에게 우스망 뎀벨레는 밉상이 되어버렸다 바르사는 7일 오전 0시 ...</td>\n",
              "      <td>2022-02-07</td>\n",
              "      <td>7일 오전 0시 15분 스페인 바르셀로나 캄노우에서 열린 아틀레티코 마드리드와 20...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7326</th>\n",
              "      <td>오피셜 성남 만 17세 유스 김지수와 준프로 계약</td>\n",
              "      <td>성남FC가 만17세 2004년생 수비수 김지수와 준프로 계약을 체결했다 김지수는 1...</td>\n",
              "      <td>2022-02-07</td>\n",
              "      <td>성남FC가 만17세 2004년생 2004년생 수비수 김지수와 준프로 계약을 체결했다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7327</th>\n",
              "      <td>오베르마스 아약스서 쫓겨난다여성 동료들에게 부적절한 메시지</td>\n",
              "      <td>레전드 마르크 오베르마스가 아약스에서 쫓겨났다 이유는 굉장히 굴욕적이었다 아약스는 ...</td>\n",
              "      <td>2022-02-07</td>\n",
              "      <td>네덜란드 출신의 레전드 마르크 오베르마스가 아약스에서 쫓겨났다 이유는 굉장히 굴욕적이었다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7328</th>\n",
              "      <td>바르사 차비 감독 트라오레 데뷔전 활약에 깜짝몸이 야수 같아</td>\n",
              "      <td>FC 바르셀로나 차비 에르난데스 감독이 데뷔전을 치른 아다마 트라오레를 극찬했다 바...</td>\n",
              "      <td>2022-02-07</td>\n",
              "      <td>지난겨울 이적시장에서 울버햄프턴을 떠나 바르사로 임대 이적했고 이날 경기에 선발 출...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7329</th>\n",
              "      <td>데파이팔카오산체스디 마리아 퍼거슨 경 은퇴 후 맨유 워스트 11</td>\n",
              "      <td>이름값만 화려한 맨유 워스트 11이 공개됐다 영국 매체 익스프레스는 7일 알렉스 퍼...</td>\n",
              "      <td>2022-02-07</td>\n",
              "      <td>영국 매체 익스프레스는 7일 알렉스 퍼거슨 경 은퇴 후 맨유의 워스트 11을 선정했...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>7330 rows × 4 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-32497e19-cb54-4d53-9b8a-caeca5dab297')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-32497e19-cb54-4d53-9b8a-caeca5dab297 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-32497e19-cb54-4d53-9b8a-caeca5dab297');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 한국어 불용어 리스트 크롤링\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.ranks.nl/stopwords/korean\"\n",
        "response = requests.get(url, verify = False)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.text,'html.parser')\n",
        "    content = soup.select_one('#article178ebefbfb1b165454ec9f168f545239 > div.panel-body > table > tbody > tr')\n",
        "    stop_words=[]\n",
        "    for x in content.strings:\n",
        "        x=x.strip()\n",
        "        if x:\n",
        "            stop_words.append(x)\n",
        "    print(f\"# Korean stop words: {len(stop_words)}\")\n",
        "else:\n",
        "    print(response.status_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y-NWBnHMOpTA",
        "outputId": "7e4444f9-08ef-49d1-c981-6d6300d0420d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/urllib3/connectionpool.py:847: InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
            "  InsecureRequestWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Korean stop words: 677\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "for i in tqdm(range(len(df))):\n",
        "  temp_data = okt.morphs(df[\"CONTENT\"].iloc[i])\n",
        "  temp_list = []\n",
        "  \n",
        "  for word in temp_data:\n",
        "    if word in stop_words: continue\n",
        "    temp_list.append(word)\n",
        "  \n",
        "  df[\"CONTENT\"].iloc[i] = \" \".join(temp_list)\n",
        "  \n",
        "  temp_data = okt.morphs(df[\"TITLE\"].iloc[i])\n",
        "  temp_list = []\n",
        "\n",
        "  for word in temp_data:\n",
        "    if word in stop_words: continue\n",
        "    temp_list.append(word)\n",
        "  \n",
        "  df[\"TITLE\"].iloc[i] = \" \".join(temp_list)\n",
        "\n",
        "  temp_data = okt.morphs(df[\"KoBART\"].iloc[i])\n",
        "  temp_list = []\n",
        "\n",
        "  for word in temp_data:\n",
        "    if word in stop_words: continue\n",
        "    temp_list.append(word)\n",
        "  \n",
        "  df[\"KoBART\"].iloc[i] = \" \".join(temp_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDFlUjpbOtiI",
        "outputId": "8c871ad2-f7ee-4b89-b53f-7bc4a113e23f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 7330/7330 [06:34<00:00, 18.58it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "_tRXcvmPRUGd",
        "outputId": "3c21a7d1-45b8-4b79-e82c-3947190cd881"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 TITLE  \\\n",
              "0         스털링 다이빙 논란 종결 오른쪽 다리 접촉 있었잖아   \n",
              "1   디 마리아 없다 유로 X 코파 베스트 11 이탈리아 만 7 명   \n",
              "2     슈퍼컴퓨터 예측 맨시티 우승 맨유 4 위 토트넘 은 6 위   \n",
              "3  이재성 완벽한 프로 마인츠 서 성공할 킬 디렉터 애정 듬뿍 응원   \n",
              "4   홈킷 딴판 바르사 팬 NEW 웨이 셔츠 호평 가장 좋아하는 색   \n",
              "\n",
              "                                             CONTENT  PUBLISH_DT  \\\n",
              "0  유럽 축구 연맹 유로 2020 심판 위원장 로베르토 로세티 잉글랜드 덴마크 전 나온...  2021-07-15   \n",
              "1  지난달 시작 된 유로 코파 아메리카 11일 끝 막 내렸다 이탈리아 는 결승전 잉글랜...  2021-07-15   \n",
              "2  새 시즌 시작 하기도 전 슈퍼컴퓨터 예상 한 순위 나왔다 영국 매체 스포츠 바이블 ...  2021-07-15   \n",
              "3  홀슈타인 킬 우베 스토 버 디렉터 이재성 향 해 응원 메시지 띄웠다 이재성 은 20...  2021-07-15   \n",
              "4  FC 바르셀로나 새 시즌 원정 유니폼 공개 했다 팬 은 만족스럽다는 반응 이다 바르...  2021-07-15   \n",
              "\n",
              "                                              KoBART  \n",
              "0  유럽 축구 연맹 유로 2020 심판 위원장 로베르토 로세티 잉글랜드 덴마크 전 나온...  \n",
              "1               지난달 시작 된 시작 된 유로 코파 아메리카 11일 끝 막 내렸다  \n",
              "2  지난 시즌 초반 고초 겪었던 맨체스터 시티 승점 88 쌓아 경쟁 팀 손쉽게 따돌리고...  \n",
              "3        홀슈타인 킬 우베 스토 버 디렉터 킬 에이스 이재성 향 해 응원 메시지 띄웠다  \n",
              "4  바르사 는 15일 공식 홈페이지 통해 20212022시 즌에 입 웨이 킷 선보였는데...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-542a8218-9034-46d7-8f50-0fe121a07618\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>TITLE</th>\n",
              "      <th>CONTENT</th>\n",
              "      <th>PUBLISH_DT</th>\n",
              "      <th>KoBART</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>스털링 다이빙 논란 종결 오른쪽 다리 접촉 있었잖아</td>\n",
              "      <td>유럽 축구 연맹 유로 2020 심판 위원장 로베르토 로세티 잉글랜드 덴마크 전 나온...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>유럽 축구 연맹 유로 2020 심판 위원장 로베르토 로세티 잉글랜드 덴마크 전 나온...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>디 마리아 없다 유로 X 코파 베스트 11 이탈리아 만 7 명</td>\n",
              "      <td>지난달 시작 된 유로 코파 아메리카 11일 끝 막 내렸다 이탈리아 는 결승전 잉글랜...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>지난달 시작 된 시작 된 유로 코파 아메리카 11일 끝 막 내렸다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>슈퍼컴퓨터 예측 맨시티 우승 맨유 4 위 토트넘 은 6 위</td>\n",
              "      <td>새 시즌 시작 하기도 전 슈퍼컴퓨터 예상 한 순위 나왔다 영국 매체 스포츠 바이블 ...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>지난 시즌 초반 고초 겪었던 맨체스터 시티 승점 88 쌓아 경쟁 팀 손쉽게 따돌리고...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>이재성 완벽한 프로 마인츠 서 성공할 킬 디렉터 애정 듬뿍 응원</td>\n",
              "      <td>홀슈타인 킬 우베 스토 버 디렉터 이재성 향 해 응원 메시지 띄웠다 이재성 은 20...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>홀슈타인 킬 우베 스토 버 디렉터 킬 에이스 이재성 향 해 응원 메시지 띄웠다</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>홈킷 딴판 바르사 팬 NEW 웨이 셔츠 호평 가장 좋아하는 색</td>\n",
              "      <td>FC 바르셀로나 새 시즌 원정 유니폼 공개 했다 팬 은 만족스럽다는 반응 이다 바르...</td>\n",
              "      <td>2021-07-15</td>\n",
              "      <td>바르사 는 15일 공식 홈페이지 통해 20212022시 즌에 입 웨이 킷 선보였는데...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-542a8218-9034-46d7-8f50-0fe121a07618')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-542a8218-9034-46d7-8f50-0fe121a07618 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-542a8218-9034-46d7-8f50-0fe121a07618');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.rename(columns={'TITLE':'summary', 'CONTENT':'news'})\n",
        "df.columns"
      ],
      "metadata": {
        "id": "rmnZl6aSPQGw",
        "outputId": "bed50b4f-84d9-45ce-e893-d05289345456",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['summary', 'news', 'PUBLISH_DT', 'KoBART'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 학습 데이터셋 구축\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df, test_size = 0.2, random_state = seed)"
      ],
      "metadata": {
        "id": "zn1Vz45tUOp5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train.to_csv('train.tsv', sep='\\t')\n",
        "test.to_csv('test.tsv', sep='\\t')"
      ],
      "metadata": {
        "id": "sTJKpUzgzMxu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/KoBART-summarization"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6kEmRR08nfB",
        "outputId": "32484d76-001c-4ee7-bc0a-375f6070b900"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/KoBART-summarization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "KcjcVK5J9G4n",
        "outputId": "9005266c-4953-48f0-b6e9-24c71dd8cef0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 1)) (1.3.5)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from -r requirements.txt (line 2)) (1.10.0+cu111)\n",
            "Collecting transformers==4.16.2\n",
            "  Downloading transformers-4.16.2-py3-none-any.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 5.3 MB/s \n",
            "\u001b[?25hCollecting pytorch-lightning==1.5.10\n",
            "  Downloading pytorch_lightning-1.5.10-py3-none-any.whl (527 kB)\n",
            "\u001b[K     |████████████████████████████████| 527 kB 54.5 MB/s \n",
            "\u001b[?25hCollecting streamlit==1.2.0\n",
            "  Downloading streamlit-1.2.0-py2.py3-none-any.whl (9.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 9.1 MB 18.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->-r requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (4.63.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (4.11.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (2.23.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (3.6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.2->-r requirements.txt (line 3)) (2019.12.20)\n",
            "Collecting tokenizers!=0.11.3,>=0.10.1\n",
            "  Downloading tokenizers-0.11.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.5 MB 34.4 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 58.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 6.5 MB/s \n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 52.1 MB/s \n",
            "\u001b[?25hCollecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.2.0-py3-none-any.whl (134 kB)\n",
            "\u001b[K     |████████████████████████████████| 134 kB 48.2 MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "  Downloading future-0.18.2.tar.gz (829 kB)\n",
            "\u001b[K     |████████████████████████████████| 829 kB 55.8 MB/s \n",
            "\u001b[?25hCollecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.7.2-py3-none-any.whl (397 kB)\n",
            "\u001b[K     |████████████████████████████████| 397 kB 58.6 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate==0.3.1\n",
            "  Downloading pyDeprecate-0.3.1-py3-none-any.whl (10 kB)\n",
            "Collecting setuptools==59.5.0\n",
            "  Downloading setuptools-59.5.0-py3-none-any.whl (952 kB)\n",
            "\u001b[K     |████████████████████████████████| 952 kB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (2.8.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (1.5.1)\n",
            "Collecting pydeck>=0.1.dev5\n",
            "  Downloading pydeck-0.7.1-py2.py3-none-any.whl (4.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.3 MB 49.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: altair>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (4.2.0)\n",
            "Collecting pympler>=0.9\n",
            "  Downloading Pympler-1.0.1-py3-none-any.whl (164 kB)\n",
            "\u001b[K     |████████████████████████████████| 164 kB 49.7 MB/s \n",
            "\u001b[?25hCollecting validators\n",
            "  Downloading validators-0.18.2-py3-none-any.whl (19 kB)\n",
            "Collecting gitpython!=3.1.19\n",
            "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
            "\u001b[K     |████████████████████████████████| 181 kB 61.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (6.0.1)\n",
            "Requirement already satisfied: tornado>=5.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (5.1.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-2.1.6-py3-none-manylinux2014_x86_64.whl (76 kB)\n",
            "\u001b[K     |████████████████████████████████| 76 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (4.2.4)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (0.8.1)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (7.1.2)\n",
            "Collecting base58\n",
            "  Downloading base58-2.1.1-py3-none-any.whl (5.6 kB)\n",
            "Collecting toml\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: protobuf!=3.11,>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (3.17.3)\n",
            "Collecting blinker\n",
            "  Downloading blinker-1.4.tar.gz (111 kB)\n",
            "\u001b[K     |████████████████████████████████| 111 kB 62.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from streamlit==1.2.0->-r requirements.txt (line 5)) (21.4.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->-r requirements.txt (line 1)) (2018.9)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.2.0->-r requirements.txt (line 5)) (0.11.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.2.0->-r requirements.txt (line 5)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.2.0->-r requirements.txt (line 5)) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from altair>=3.2.0->streamlit==1.2.0->-r requirements.txt (line 5)) (2.11.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 47.8 MB/s \n",
            "\u001b[?25hCollecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.2.0->-r requirements.txt (line 5)) (5.4.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0->altair>=3.2.0->streamlit==1.2.0->-r requirements.txt (line 5)) (0.18.1)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair>=3.2.0->streamlit==1.2.0->-r requirements.txt (line 5)) (3.7.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.2->-r requirements.txt (line 3)) (3.0.7)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf!=3.11,>=3.6.0->streamlit==1.2.0->-r requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: ipywidgets>=7.0.0 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (7.6.5)\n",
            "Collecting ipykernel>=5.1.2\n",
            "  Downloading ipykernel-6.9.2-py3-none-any.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 62.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: traitlets>=4.3.2 in /usr/local/lib/python3.7/dist-packages (from pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (5.1.1)\n",
            "Requirement already satisfied: jupyter-client<8.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (5.3.5)\n",
            "Requirement already satisfied: debugpy<2.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (1.0.0)\n",
            "Collecting ipython>=7.23.1\n",
            "  Downloading ipython-7.32.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 58.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (5.4.8)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (1.5.4)\n",
            "Requirement already satisfied: matplotlib-inline<0.2.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.1.3)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.18.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (2.6.1)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.28-py3-none-any.whl (380 kB)\n",
            "\u001b[K     |████████████████████████████████| 380 kB 60.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.7.5)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (4.8.0)\n",
            "Requirement already satisfied: nbformat>=4.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (5.1.3)\n",
            "Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.2.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (3.5.2)\n",
            "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (1.0.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.8.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->altair>=3.2.0->streamlit==1.2.0->-r requirements.txt (line 5)) (2.0.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (22.3.0)\n",
            "Requirement already satisfied: jupyter-core>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-client<8.0->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (4.9.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.23.1->ipykernel>=5.1.2->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.2.5)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (1.35.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (1.8.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (3.3.6)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (0.37.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (1.44.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (1.0.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (1.0.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (0.6.1)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (0.4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (1.3.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (0.4.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.2->-r requirements.txt (line 3)) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (3.2.0)\n",
            "Requirement already satisfied: notebook>=4.4.1 in /usr/local/lib/python3.7/dist-packages (from widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (5.3.1)\n",
            "Requirement already satisfied: terminado>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.13.3)\n",
            "Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (1.8.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.7/dist-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (5.6.1)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch-lightning==1.5.10->-r requirements.txt (line 4)) (2.0.12)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.0 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 63.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (1.5.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (4.1.0)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.8.4)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.6.0)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.7.1)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets>=7.0.0->pydeck>=0.1.dev5->streamlit==1.2.0->-r requirements.txt (line 5)) (0.5.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.2->-r requirements.txt (line 3)) (1.1.0)\n",
            "Building wheels for collected packages: future, blinker\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=1e33d41e89a99f600b4456f9199cd77c19b8f82382dc4e88beeb4643b036ba70\n",
            "  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
            "  Building wheel for blinker (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for blinker: filename=blinker-1.4-py3-none-any.whl size=13478 sha256=535ef5226a8c40934bf711c181e5545762fd8e5fa3fa87a44351f2ccdb03eeec\n",
            "  Stored in directory: /root/.cache/pip/wheels/22/f5/18/df711b66eb25b21325c132757d4314db9ac5e8dabeaf196eab\n",
            "Successfully built future blinker\n",
            "Installing collected packages: setuptools, prompt-toolkit, ipython, ipykernel, multidict, frozenlist, yarl, smmap, asynctest, async-timeout, aiosignal, pyyaml, pyDeprecate, gitdb, fsspec, aiohttp, watchdog, validators, torchmetrics, toml, tokenizers, sacremoses, pympler, pydeck, huggingface-hub, gitpython, future, blinker, base58, transformers, streamlit, pytorch-lightning\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 57.4.0\n",
            "    Uninstalling setuptools-57.4.0:\n",
            "      Successfully uninstalled setuptools-57.4.0\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "  Attempting uninstall: ipykernel\n",
            "    Found existing installation: ipykernel 4.10.1\n",
            "    Uninstalling ipykernel-4.10.1:\n",
            "      Successfully uninstalled ipykernel-4.10.1\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Attempting uninstall: future\n",
            "    Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.28 which is incompatible.\n",
            "google-colab 1.0.0 requires ipykernel~=4.10, but you have ipykernel 6.9.2 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.32.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 base58-2.1.1 blinker-1.4 frozenlist-1.3.0 fsspec-2022.2.0 future-0.18.2 gitdb-4.0.9 gitpython-3.1.27 huggingface-hub-0.4.0 ipykernel-6.9.2 ipython-7.32.0 multidict-6.0.2 prompt-toolkit-3.0.28 pyDeprecate-0.3.1 pydeck-0.7.1 pympler-1.0.1 pytorch-lightning-1.5.10 pyyaml-6.0 sacremoses-0.0.49 setuptools-59.5.0 smmap-5.0.0 streamlit-1.2.0 tokenizers-0.11.6 toml-0.10.2 torchmetrics-0.7.2 transformers-4.16.2 validators-0.18.2 watchdog-2.1.6 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "ipykernel",
                  "pkg_resources",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! mkdir logs"
      ],
      "metadata": {
        "id": "cYUfqcTC9VWD"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! python train.py  --gradient_clip_val 1.0  \\\n",
        "                 --max_epochs 50 \\\n",
        "                 --default_root_dir logs \\\n",
        "                 --gpus 1 \\\n",
        "                 --batch_size 4 \\\n",
        "                 --num_workers 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04aHn3HA8o_9",
        "outputId": "dd70f6e4-dea4-4ecd-b1a8-ef9f8a4c51d2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: 100% 4.00/4.00 [00:00<00:00, 3.62kB/s]\n",
            "Downloading: 100% 111/111 [00:00<00:00, 96.2kB/s]\n",
            "Downloading: 100% 666k/666k [00:00<00:00, 4.71MB/s]\n",
            "Downloading: 100% 1.33k/1.33k [00:00<00:00, 1.06MB/s]\n",
            "INFO:root:Namespace(accelerator=None, accumulate_grad_batches=None, amp_backend='native', amp_level=None, auto_lr_find=False, auto_scale_batch_size=False, auto_select_gpus=False, batch_size=4, benchmark=False, check_val_every_n_epoch=1, checkpoint_callback=None, checkpoint_path=None, default_root_dir='logs', detect_anomaly=False, deterministic=False, devices=None, enable_checkpointing=True, enable_model_summary=True, enable_progress_bar=True, fast_dev_run=False, flush_logs_every_n_steps=None, gpus=1, gradient_clip_algorithm=None, gradient_clip_val=1.0, ipus=None, limit_predict_batches=1.0, limit_test_batches=1.0, limit_train_batches=1.0, limit_val_batches=1.0, log_every_n_steps=50, log_gpu_memory=None, logger=True, lr=3e-05, max_epochs=50, max_len=512, max_steps=-1, max_time=None, min_epochs=None, min_steps=None, model_path=None, move_metrics_to_cpu=False, multiple_trainloader_mode='max_size_cycle', num_nodes=1, num_processes=1, num_sanity_val_steps=2, num_workers=4, overfit_batches=0.0, plugins=None, precision=32, prepare_data_per_node=None, process_position=0, profiler=None, progress_bar_refresh_rate=None, reload_dataloaders_every_epoch=False, reload_dataloaders_every_n_epochs=0, replace_sampler_ddp=True, resume_from_checkpoint=None, stochastic_weight_avg=False, strategy=None, sync_batchnorm=False, terminate_on_nan=None, test_file='data/test.tsv', tpu_cores=None, track_grad_norm=-1, train_file='data/train.tsv', val_check_interval=1.0, warmup_ratio=0.1, weights_save_path=None, weights_summary='top')\n",
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "Downloading: 100% 473M/473M [00:08<00:00, 60.0MB/s]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "INFO:root:number of workers 4, data length 1466\n",
            "INFO:root:num_train_steps : 4581\n",
            "INFO:root:num_warmup_steps : 458\n",
            "\n",
            "  | Name  | Type                         | Params\n",
            "-------------------------------------------------------\n",
            "0 | model | BartForConditionalGeneration | 123 M \n",
            "-------------------------------------------------------\n",
            "123 M     Trainable params\n",
            "0         Non-trainable params\n",
            "123 M     Total params\n",
            "495.440   Total estimated model params size (MB)\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py:631: UserWarning: Checkpoint directory /content/KoBART-summarization/logs exists and is not empty.\n",
            "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
            "Epoch 0:  81% 1480/1833 [08:44<02:04,  2.82it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  82% 1500/1833 [08:46<01:56,  2.85it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  83% 1520/1833 [08:48<01:48,  2.87it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  84% 1540/1833 [08:51<01:41,  2.90it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  85% 1560/1833 [08:53<01:33,  2.92it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  86% 1580/1833 [08:55<01:25,  2.95it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  88% 1620/1833 [09:00<01:11,  3.00it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  89% 1640/1833 [09:02<01:03,  3.02it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  91% 1660/1833 [09:05<00:56,  3.05it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  92% 1680/1833 [09:07<00:49,  3.07it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  93% 1700/1833 [09:09<00:43,  3.09it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  94% 1720/1833 [09:12<00:36,  3.12it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  95% 1740/1833 [09:14<00:29,  3.14it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  96% 1760/1833 [09:16<00:23,  3.16it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  98% 1800/1833 [09:21<00:10,  3.21it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Epoch 0:  99% 1820/1833 [09:23<00:04,  3.23it/s, loss=2.67, v_num=0, train_loss=2.550]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.59it/s]\u001b[A\n",
            "Epoch 0: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=2.62, v_num=0, train_loss=2.310, val_loss=2.760]\n",
            "Epoch 0: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=2.62, v_num=0, train_loss=2.310, val_loss=2.760]Epoch 0, global step 1465: val_loss reached 2.76135 (best 2.76135), saving model to \"/content/KoBART-summarization/logs/model_chp/epoch=00-val_loss=2.761.ckpt\" as top 3\n",
            "tcmalloc: large alloc 1075716096 bytes == 0x55db0e660000 @  0x7f4ef489c615 0x55d9b9cd73bc 0x55d9b9db818a 0x55d9b9cdde0c 0x7f4ee0e819e4 0x7f4ee0e89b14 0x7f4ee0e5ea60 0x7f4e3863cf55 0x7f4e3863888e 0x7f4e38640235 0x7f4ee0e5efae 0x7f4ee05d5aa8 0x55d9b9cdaf78 0x55d9b9d4ea6d 0x55d9b9d4966e 0x55d9b9cdbaba 0x55d9b9d49eae 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4e2c0 0x55d9b9cdb9da 0x55d9b9d49eae 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4a108 0x55d9b9cdb9da 0x55d9b9d4a108 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4a108 0x55d9b9d4902f\n",
            "tcmalloc: large alloc 1344651264 bytes == 0x55dab211c000 @  0x7f4ef489c615 0x55d9b9cd73bc 0x55d9b9db818a 0x55d9b9cdde0c 0x7f4ee0e819e4 0x7f4ee0e89b14 0x7f4ee0e5ea60 0x7f4e3863cf55 0x7f4e3863888e 0x7f4e38640235 0x7f4ee0e5efae 0x7f4ee05d5aa8 0x55d9b9cdaf78 0x55d9b9d4ea6d 0x55d9b9d4966e 0x55d9b9cdbaba 0x55d9b9d49eae 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4e2c0 0x55d9b9cdb9da 0x55d9b9d49eae 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4a108 0x55d9b9cdb9da 0x55d9b9d4a108 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4a108 0x55d9b9d4902f\n",
            "tcmalloc: large alloc 1680818176 bytes == 0x55db02378000 @  0x7f4ef489c615 0x55d9b9cd73bc 0x55d9b9db818a 0x55d9b9cdde0c 0x7f4ee0e819e4 0x7f4ee0e89b14 0x7f4ee0e5ea60 0x7f4e3863cf55 0x7f4e3863888e 0x7f4e38640235 0x7f4ee0e5efae 0x7f4ee05d5aa8 0x55d9b9cdaf78 0x55d9b9d4ea6d 0x55d9b9d4966e 0x55d9b9cdbaba 0x55d9b9d49eae 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4e2c0 0x55d9b9cdb9da 0x55d9b9d49eae 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4a108 0x55d9b9cdb9da 0x55d9b9d4a108 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4a108 0x55d9b9d4902f\n",
            "Epoch 0: 100% 1833/1833 [09:35<00:00,  3.18it/s, loss=2.62, v_num=0, train_loss=2.310, val_loss=2.760]tcmalloc: large alloc 1680818176 bytes == 0x55db02378000 @  0x7f4ef489c615 0x55d9b9cd73bc 0x55d9b9db818a 0x55d9b9cdde0c 0x7f4ee0e819e4 0x7f4ee0e89b14 0x7f4ee0e5ea60 0x7f4e3863cf55 0x7f4e3863888e 0x7f4e38640235 0x7f4ee0e5efae 0x7f4ee05d5aa8 0x55d9b9cdaf78 0x55d9b9d4ea6d 0x55d9b9d4966e 0x55d9b9cdbaba 0x55d9b9d49eae 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4e2c0 0x55d9b9cdb9da 0x55d9b9d49eae 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4a108 0x55d9b9cdb9da 0x55d9b9d4a108 0x55d9b9d4902f 0x55d9b9cdbaba 0x55d9b9d4a108 0x55d9b9d4902f\n",
            "Epoch 1:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 1:  82% 1500/1833 [08:46<01:56,  2.85it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  83% 1520/1833 [08:49<01:48,  2.87it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  84% 1540/1833 [08:51<01:41,  2.90it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  85% 1560/1833 [08:53<01:33,  2.92it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  86% 1580/1833 [08:56<01:25,  2.95it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  88% 1620/1833 [09:00<01:11,  2.99it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  92% 1680/1833 [09:07<00:49,  3.07it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  95% 1740/1833 [09:14<00:29,  3.14it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  98% 1800/1833 [09:21<00:10,  3.20it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Epoch 1:  99% 1820/1833 [09:24<00:04,  3.23it/s, loss=2.06, v_num=0, train_loss=1.860, val_loss=2.760]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 1: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=2.07, v_num=0, train_loss=1.620, val_loss=2.660]\n",
            "Epoch 1: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=2.07, v_num=0, train_loss=1.620, val_loss=2.660]Epoch 1, global step 2931: val_loss reached 2.66022 (best 2.66022), saving model to \"/content/KoBART-summarization/logs/model_chp/epoch=01-val_loss=2.660.ckpt\" as top 3\n",
            "Epoch 2:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  82% 1500/1833 [08:46<01:56,  2.85it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  83% 1520/1833 [08:49<01:48,  2.87it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  84% 1540/1833 [08:51<01:41,  2.90it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  85% 1560/1833 [08:53<01:33,  2.92it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  86% 1580/1833 [08:56<01:25,  2.95it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  88% 1620/1833 [09:00<01:11,  3.00it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  92% 1680/1833 [09:07<00:49,  3.07it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  95% 1740/1833 [09:14<00:29,  3.14it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  98% 1800/1833 [09:21<00:10,  3.20it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Epoch 2:  99% 1820/1833 [09:24<00:04,  3.23it/s, loss=1.5, v_num=0, train_loss=1.130, val_loss=2.660]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 2: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.5, v_num=0, train_loss=1.090, val_loss=2.740]\n",
            "Epoch 2: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.5, v_num=0, train_loss=1.090, val_loss=2.740]Epoch 2, global step 4397: val_loss reached 2.74348 (best 2.66022), saving model to \"/content/KoBART-summarization/logs/model_chp/epoch=02-val_loss=2.743.ckpt\" as top 3\n",
            "Epoch 3:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 3:  82% 1500/1833 [08:46<01:56,  2.85it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  83% 1520/1833 [08:49<01:48,  2.87it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  84% 1540/1833 [08:51<01:41,  2.90it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  85% 1560/1833 [08:53<01:33,  2.92it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  86% 1580/1833 [08:56<01:25,  2.95it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  88% 1620/1833 [09:00<01:11,  2.99it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  92% 1680/1833 [09:07<00:49,  3.07it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  95% 1740/1833 [09:14<00:29,  3.14it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  98% 1800/1833 [09:21<00:10,  3.20it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Epoch 3:  99% 1820/1833 [09:24<00:04,  3.23it/s, loss=1.27, v_num=0, train_loss=1.430, val_loss=2.740]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 3: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.23, v_num=0, train_loss=1.270, val_loss=2.790]\n",
            "Epoch 3: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.23, v_num=0, train_loss=1.270, val_loss=2.790]Epoch 3, global step 5863: val_loss was not in top 3\n",
            "Epoch 4:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  82% 1500/1833 [08:47<01:57,  2.85it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  84% 1540/1833 [08:51<01:41,  2.90it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  86% 1580/1833 [08:56<01:25,  2.95it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Epoch 4:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=1.57, v_num=0, train_loss=1.960, val_loss=2.790]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 4: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.6, v_num=0, train_loss=1.130, val_loss=2.860] \n",
            "Epoch 4: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.6, v_num=0, train_loss=1.130, val_loss=2.860]Epoch 4, global step 7329: val_loss was not in top 3\n",
            "Epoch 5:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 5:  82% 1500/1833 [08:47<01:57,  2.85it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  84% 1540/1833 [08:51<01:41,  2.90it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Epoch 5:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=1.48, v_num=0, train_loss=1.150, val_loss=2.860]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 5: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.42, v_num=0, train_loss=1.100, val_loss=2.940]\n",
            "Epoch 5: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.42, v_num=0, train_loss=1.100, val_loss=2.940]Epoch 5, global step 8795: val_loss was not in top 3\n",
            "Epoch 6:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 6:  82% 1500/1833 [08:47<01:57,  2.85it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  84% 1540/1833 [08:51<01:41,  2.90it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  86% 1580/1833 [08:56<01:25,  2.95it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  92% 1680/1833 [09:08<00:49,  3.07it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Epoch 6:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=1.09, v_num=0, train_loss=1.350, val_loss=2.940]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 6: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.07, v_num=0, train_loss=0.752, val_loss=3.080]\n",
            "Epoch 6: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=1.07, v_num=0, train_loss=0.752, val_loss=3.080]Epoch 6, global step 10261: val_loss was not in top 3\n",
            "Epoch 7:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 7:  82% 1500/1833 [08:47<01:57,  2.85it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  84% 1540/1833 [08:51<01:41,  2.90it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Epoch 7:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.497, v_num=0, train_loss=0.189, val_loss=3.080]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 7: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.518, v_num=0, train_loss=0.341, val_loss=3.190]\n",
            "Epoch 7: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.518, v_num=0, train_loss=0.341, val_loss=3.190]Epoch 7, global step 11727: val_loss was not in top 3\n",
            "Epoch 8:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 8:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  87% 1600/1833 [08:59<01:18,  2.97it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  91% 1660/1833 [09:06<00:56,  3.04it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  94% 1720/1833 [09:13<00:36,  3.11it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  97% 1780/1833 [09:20<00:16,  3.18it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Epoch 8:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.389, v_num=0, train_loss=0.438, val_loss=3.190]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 8: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.387, v_num=0, train_loss=0.278, val_loss=3.240]\n",
            "Epoch 8: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.387, v_num=0, train_loss=0.278, val_loss=3.240]Epoch 8, global step 13193: val_loss was not in top 3\n",
            "Epoch 9:  81% 1480/1833 [08:45<02:05,  2.82it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 9:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  83% 1520/1833 [08:50<01:49,  2.87it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  86% 1580/1833 [08:57<01:26,  2.94it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  87% 1600/1833 [08:59<01:18,  2.97it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  89% 1640/1833 [09:04<01:04,  3.01it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  91% 1660/1833 [09:06<00:56,  3.04it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  93% 1700/1833 [09:11<00:43,  3.08it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  94% 1720/1833 [09:13<00:36,  3.11it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  96% 1760/1833 [09:18<00:23,  3.15it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  97% 1780/1833 [09:20<00:16,  3.18it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Epoch 9:  99% 1820/1833 [09:25<00:04,  3.22it/s, loss=0.328, v_num=0, train_loss=0.250, val_loss=3.240]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 9: 100% 1833/1833 [09:28<00:00,  3.22it/s, loss=0.326, v_num=0, train_loss=0.359, val_loss=3.300]\n",
            "Epoch 9: 100% 1833/1833 [09:28<00:00,  3.22it/s, loss=0.326, v_num=0, train_loss=0.359, val_loss=3.300]Epoch 9, global step 14659: val_loss was not in top 3\n",
            "Epoch 10:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 10:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  87% 1600/1833 [08:59<01:18,  2.97it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  91% 1660/1833 [09:06<00:56,  3.04it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  94% 1720/1833 [09:13<00:36,  3.11it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  97% 1780/1833 [09:20<00:16,  3.18it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Epoch 10:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.446, v_num=0, train_loss=0.633, val_loss=3.300]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 10: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.467, v_num=0, train_loss=0.393, val_loss=3.410]\n",
            "Epoch 10: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.467, v_num=0, train_loss=0.393, val_loss=3.410]Epoch 10, global step 16125: val_loss was not in top 3\n",
            "Epoch 11:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  87% 1600/1833 [08:59<01:18,  2.97it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  91% 1660/1833 [09:06<00:56,  3.04it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  94% 1720/1833 [09:13<00:36,  3.11it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  97% 1780/1833 [09:20<00:16,  3.18it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Epoch 11:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.42, v_num=0, train_loss=0.220, val_loss=3.410]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 11: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.382, v_num=0, train_loss=0.458, val_loss=3.480]\n",
            "Epoch 11: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.382, v_num=0, train_loss=0.458, val_loss=3.480]Epoch 11, global step 17591: val_loss was not in top 3\n",
            "Epoch 12:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 12:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Epoch 12:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.207, v_num=0, train_loss=0.213, val_loss=3.480]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 12: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.193, v_num=0, train_loss=0.290, val_loss=3.530]\n",
            "Epoch 12: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.193, v_num=0, train_loss=0.290, val_loss=3.530]Epoch 12, global step 19057: val_loss was not in top 3\n",
            "Epoch 13:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 13:  82% 1500/1833 [08:47<01:57,  2.85it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  84% 1540/1833 [08:51<01:41,  2.90it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Epoch 13:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.0912, v_num=0, train_loss=0.114, val_loss=3.530]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 13: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.101, v_num=0, train_loss=0.0522, val_loss=3.600]\n",
            "Epoch 13: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.101, v_num=0, train_loss=0.0522, val_loss=3.600]Epoch 13, global step 20523: val_loss was not in top 3\n",
            "Epoch 14:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 14:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  87% 1600/1833 [08:59<01:18,  2.97it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  91% 1660/1833 [09:06<00:56,  3.04it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Epoch 14:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.0685, v_num=0, train_loss=0.0614, val_loss=3.600]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 14: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.0685, v_num=0, train_loss=0.0512, val_loss=3.630]\n",
            "Epoch 14: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.0685, v_num=0, train_loss=0.0512, val_loss=3.630]Epoch 14, global step 21989: val_loss was not in top 3\n",
            "Epoch 15:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 15:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  84% 1540/1833 [08:51<01:41,  2.89it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Epoch 15:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.0987, v_num=0, train_loss=0.143, val_loss=3.630]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 15: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.101, v_num=0, train_loss=0.0585, val_loss=3.700]\n",
            "Epoch 15: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.101, v_num=0, train_loss=0.0585, val_loss=3.700]Epoch 15, global step 23455: val_loss was not in top 3\n",
            "Epoch 16:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 16:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  87% 1600/1833 [08:59<01:18,  2.97it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  91% 1660/1833 [09:06<00:56,  3.04it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  94% 1720/1833 [09:13<00:36,  3.11it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  97% 1780/1833 [09:20<00:16,  3.18it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Epoch 16:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.143, v_num=0, train_loss=0.110, val_loss=3.700]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 16: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.153, v_num=0, train_loss=0.104, val_loss=3.750]\n",
            "Epoch 16: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.153, v_num=0, train_loss=0.104, val_loss=3.750]Epoch 16, global step 24921: val_loss was not in top 3\n",
            "Epoch 17:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 17:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  87% 1600/1833 [08:59<01:18,  2.97it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  91% 1660/1833 [09:06<00:56,  3.04it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  94% 1720/1833 [09:13<00:36,  3.11it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  97% 1780/1833 [09:20<00:16,  3.18it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Epoch 17:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.139, v_num=0, train_loss=0.198, val_loss=3.750]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 17: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.135, v_num=0, train_loss=0.0722, val_loss=3.800]\n",
            "Epoch 17: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.135, v_num=0, train_loss=0.0722, val_loss=3.800]Epoch 17, global step 26387: val_loss was not in top 3\n",
            "Epoch 18:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 18:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  87% 1600/1833 [08:59<01:18,  2.97it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  89% 1640/1833 [09:03<01:04,  3.02it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  91% 1660/1833 [09:06<00:56,  3.04it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  94% 1720/1833 [09:13<00:36,  3.11it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  96% 1760/1833 [09:17<00:23,  3.15it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  97% 1780/1833 [09:20<00:16,  3.18it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Epoch 18:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.0719, v_num=0, train_loss=0.0359, val_loss=3.800]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.58it/s]\u001b[A\n",
            "Epoch 18: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.0668, v_num=0, train_loss=0.030, val_loss=3.810] \n",
            "Epoch 18: 100% 1833/1833 [09:28<00:00,  3.23it/s, loss=0.0668, v_num=0, train_loss=0.030, val_loss=3.810]Epoch 18, global step 27853: val_loss was not in top 3\n",
            "Epoch 19:  81% 1480/1833 [08:44<02:05,  2.82it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 19:  82% 1500/1833 [08:47<01:57,  2.84it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  83% 1520/1833 [08:49<01:49,  2.87it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  84% 1540/1833 [08:52<01:41,  2.89it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  85% 1560/1833 [08:54<01:33,  2.92it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  86% 1580/1833 [08:56<01:25,  2.94it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  87% 1600/1833 [08:58<01:18,  2.97it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  88% 1620/1833 [09:01<01:11,  2.99it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  89% 1640/1833 [09:03<01:03,  3.02it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  91% 1660/1833 [09:05<00:56,  3.04it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  92% 1680/1833 [09:08<00:49,  3.06it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  93% 1700/1833 [09:10<00:43,  3.09it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  94% 1720/1833 [09:12<00:36,  3.11it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  95% 1740/1833 [09:15<00:29,  3.13it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  96% 1760/1833 [09:17<00:23,  3.16it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  97% 1780/1833 [09:19<00:16,  3.18it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  98% 1800/1833 [09:22<00:10,  3.20it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Epoch 19:  99% 1820/1833 [09:24<00:04,  3.22it/s, loss=0.0295, v_num=0, train_loss=0.0207, val_loss=3.810]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 19: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.0299, v_num=0, train_loss=0.0395, val_loss=3.880]\n",
            "Epoch 19: 100% 1833/1833 [09:27<00:00,  3.23it/s, loss=0.0299, v_num=0, train_loss=0.0395, val_loss=3.880]Epoch 19, global step 29319: val_loss was not in top 3\n",
            "Epoch 20:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 20:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Epoch 20:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0245, v_num=0, train_loss=0.0101, val_loss=3.880]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 20: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0244, v_num=0, train_loss=0.0268, val_loss=3.910]\n",
            "Epoch 20: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0244, v_num=0, train_loss=0.0268, val_loss=3.910]Epoch 20, global step 30785: val_loss was not in top 3\n",
            "Epoch 21:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 21:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Epoch 21:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0416, v_num=0, train_loss=0.0589, val_loss=3.910]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.62it/s]\u001b[A\n",
            "Epoch 21: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.055, v_num=0, train_loss=0.0619, val_loss=4.010] \n",
            "Epoch 21: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.055, v_num=0, train_loss=0.0619, val_loss=4.010]Epoch 21, global step 32251: val_loss was not in top 3\n",
            "Epoch 22:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 22:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  93% 1700/1833 [09:07<00:42,  3.11it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Epoch 22:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0875, v_num=0, train_loss=0.0899, val_loss=4.010]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.62it/s]\u001b[A\n",
            "Epoch 22: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.1, v_num=0, train_loss=0.168, val_loss=4.040]    \n",
            "Epoch 22: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.1, v_num=0, train_loss=0.168, val_loss=4.040]Epoch 22, global step 33717: val_loss was not in top 3\n",
            "Epoch 23:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 23:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Epoch 23:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0532, v_num=0, train_loss=0.142, val_loss=4.040]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 23: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0507, v_num=0, train_loss=0.0528, val_loss=4.010]\n",
            "Epoch 23: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0507, v_num=0, train_loss=0.0528, val_loss=4.010]Epoch 23, global step 35183: val_loss was not in top 3\n",
            "Epoch 24:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 24:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  93% 1700/1833 [09:07<00:42,  3.11it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Epoch 24:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0228, v_num=0, train_loss=0.0198, val_loss=4.010]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 24: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0216, v_num=0, train_loss=0.0152, val_loss=4.040]\n",
            "Epoch 24: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0216, v_num=0, train_loss=0.0152, val_loss=4.040]Epoch 24, global step 36649: val_loss was not in top 3\n",
            "Epoch 25:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 25:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Epoch 25:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.00864, v_num=0, train_loss=0.00305, val_loss=4.040]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 25: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00967, v_num=0, train_loss=0.00778, val_loss=4.060]\n",
            "Epoch 25: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00967, v_num=0, train_loss=0.00778, val_loss=4.060]Epoch 25, global step 38115: val_loss was not in top 3\n",
            "Epoch 26:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 26:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Epoch 26:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0152, v_num=0, train_loss=0.0264, val_loss=4.060]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 26: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0159, v_num=0, train_loss=0.0251, val_loss=4.180]\n",
            "Epoch 26: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0159, v_num=0, train_loss=0.0251, val_loss=4.180]Epoch 26, global step 39581: val_loss was not in top 3\n",
            "Epoch 27:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 27:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  93% 1700/1833 [09:07<00:42,  3.11it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Epoch 27:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0451, v_num=0, train_loss=0.0477, val_loss=4.180]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.62it/s]\u001b[A\n",
            "Epoch 27: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0573, v_num=0, train_loss=0.0395, val_loss=4.140]\n",
            "Epoch 27: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0573, v_num=0, train_loss=0.0395, val_loss=4.140]Epoch 27, global step 41047: val_loss was not in top 3\n",
            "Epoch 28:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 28:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  93% 1700/1833 [09:07<00:42,  3.11it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Epoch 28:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0602, v_num=0, train_loss=0.0903, val_loss=4.140]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 28: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0657, v_num=0, train_loss=0.0218, val_loss=4.220]\n",
            "Epoch 28: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0657, v_num=0, train_loss=0.0218, val_loss=4.220]Epoch 28, global step 42513: val_loss was not in top 3\n",
            "Epoch 29:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 29:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  93% 1700/1833 [09:07<00:42,  3.11it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Epoch 29:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0295, v_num=0, train_loss=0.0351, val_loss=4.220]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 29: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0258, v_num=0, train_loss=0.0163, val_loss=4.170]\n",
            "Epoch 29: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0258, v_num=0, train_loss=0.0163, val_loss=4.170]Epoch 29, global step 43979: val_loss was not in top 3\n",
            "Epoch 30:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 30:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  93% 1700/1833 [09:07<00:42,  3.11it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Epoch 30:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0115, v_num=0, train_loss=0.00282, val_loss=4.170]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 30: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0102, v_num=0, train_loss=0.00608, val_loss=4.190]\n",
            "Epoch 30: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0102, v_num=0, train_loss=0.00608, val_loss=4.190]Epoch 30, global step 45445: val_loss was not in top 3\n",
            "Epoch 31:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 31:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  93% 1700/1833 [09:07<00:42,  3.11it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Epoch 31:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0111, v_num=0, train_loss=0.0142, val_loss=4.190]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 31: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0128, v_num=0, train_loss=0.00961, val_loss=4.230]\n",
            "Epoch 31: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0128, v_num=0, train_loss=0.00961, val_loss=4.230]Epoch 31, global step 46911: val_loss was not in top 3\n",
            "Epoch 32:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 32:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  93% 1700/1833 [09:07<00:42,  3.11it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Epoch 32:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0146, v_num=0, train_loss=0.00867, val_loss=4.230]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 32: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0268, v_num=0, train_loss=0.0149, val_loss=4.250] \n",
            "Epoch 32: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0268, v_num=0, train_loss=0.0149, val_loss=4.250]Epoch 32, global step 48377: val_loss was not in top 3\n",
            "Epoch 33:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 33:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  87% 1600/1833 [08:56<01:18,  2.99it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Epoch 33:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0416, v_num=0, train_loss=0.0332, val_loss=4.250]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 33: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0392, v_num=0, train_loss=0.0269, val_loss=4.310]\n",
            "Epoch 33: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0392, v_num=0, train_loss=0.0269, val_loss=4.310]Epoch 33, global step 49843: val_loss was not in top 3\n",
            "Epoch 34:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 34:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Epoch 34:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0361, v_num=0, train_loss=0.0651, val_loss=4.310]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 34: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0308, v_num=0, train_loss=0.0236, val_loss=4.320]\n",
            "Epoch 34: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0308, v_num=0, train_loss=0.0236, val_loss=4.320]Epoch 34, global step 51309: val_loss was not in top 3\n",
            "Epoch 35:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 35:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  87% 1600/1833 [08:56<01:18,  2.98it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  91% 1660/1833 [09:03<00:56,  3.06it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Epoch 35:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0119, v_num=0, train_loss=0.00827, val_loss=4.320]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 35: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.0264, v_num=0, train_loss=0.00147, val_loss=4.300]\n",
            "Epoch 35: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.0264, v_num=0, train_loss=0.00147, val_loss=4.300]Epoch 35, global step 52775: val_loss was not in top 3\n",
            "Epoch 36:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 36:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  93% 1700/1833 [09:07<00:42,  3.11it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  96% 1760/1833 [09:14<00:22,  3.17it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Epoch 36:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.00646, v_num=0, train_loss=0.00237, val_loss=4.300]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 36: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00414, v_num=0, train_loss=0.00372, val_loss=4.310]\n",
            "Epoch 36: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00414, v_num=0, train_loss=0.00372, val_loss=4.310]Epoch 36, global step 54241: val_loss was not in top 3\n",
            "Epoch 37:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 37:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  87% 1600/1833 [08:56<01:18,  2.98it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Epoch 37:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0064, v_num=0, train_loss=0.00325, val_loss=4.310]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 37: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00609, v_num=0, train_loss=0.00543, val_loss=4.310]\n",
            "Epoch 37: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00609, v_num=0, train_loss=0.00543, val_loss=4.310]Epoch 37, global step 55707: val_loss was not in top 3\n",
            "Epoch 38:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 38:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Epoch 38:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0258, v_num=0, train_loss=0.00342, val_loss=4.310]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 38: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0231, v_num=0, train_loss=0.0154, val_loss=4.430] \n",
            "Epoch 38: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0231, v_num=0, train_loss=0.0154, val_loss=4.430]Epoch 38, global step 57173: val_loss was not in top 3\n",
            "Epoch 39:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 39:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  85% 1560/1833 [08:51<01:33,  2.94it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  87% 1600/1833 [08:56<01:18,  2.98it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  91% 1660/1833 [09:03<00:56,  3.06it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  94% 1720/1833 [09:10<00:36,  3.13it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Epoch 39:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.044, v_num=0, train_loss=0.0361, val_loss=4.430]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 39: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.0345, v_num=0, train_loss=0.0196, val_loss=4.420]\n",
            "Epoch 39: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.0345, v_num=0, train_loss=0.0196, val_loss=4.420]Epoch 39, global step 58639: val_loss was not in top 3\n",
            "Epoch 40:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 40:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  87% 1600/1833 [08:56<01:18,  2.99it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Epoch 40:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0221, v_num=0, train_loss=0.00572, val_loss=4.420]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 40: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0206, v_num=0, train_loss=0.00946, val_loss=4.430]\n",
            "Epoch 40: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0206, v_num=0, train_loss=0.00946, val_loss=4.430]Epoch 40, global step 60105: val_loss was not in top 3\n",
            "Epoch 41:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 41:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  85% 1560/1833 [08:51<01:33,  2.94it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  87% 1600/1833 [08:56<01:18,  2.98it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  91% 1660/1833 [09:03<00:56,  3.06it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  94% 1720/1833 [09:10<00:36,  3.13it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Epoch 41:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.00647, v_num=0, train_loss=0.00103, val_loss=4.430]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 41: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.00495, v_num=0, train_loss=0.00217, val_loss=4.390]\n",
            "Epoch 41: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.00495, v_num=0, train_loss=0.00217, val_loss=4.390]Epoch 41, global step 61571: val_loss was not in top 3\n",
            "Epoch 42:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 42:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  87% 1600/1833 [08:56<01:18,  2.99it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Epoch 42:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.00341, v_num=0, train_loss=0.0105, val_loss=4.390]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 42: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00448, v_num=0, train_loss=0.026, val_loss=4.400] \n",
            "Epoch 42: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00448, v_num=0, train_loss=0.026, val_loss=4.400]Epoch 42, global step 63037: val_loss was not in top 3\n",
            "Epoch 43:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 43:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Epoch 43:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.00553, v_num=0, train_loss=0.00658, val_loss=4.400]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 43: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0103, v_num=0, train_loss=0.00455, val_loss=4.470] \n",
            "Epoch 43: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.0103, v_num=0, train_loss=0.00455, val_loss=4.470]Epoch 43, global step 64503: val_loss was not in top 3\n",
            "Epoch 44:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 44:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  87% 1600/1833 [08:56<01:18,  2.98it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Epoch 44:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.025, v_num=0, train_loss=0.00478, val_loss=4.470]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 44: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.0215, v_num=0, train_loss=0.0103, val_loss=4.480]\n",
            "Epoch 44: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.0215, v_num=0, train_loss=0.0103, val_loss=4.480]Epoch 44, global step 65969: val_loss was not in top 3\n",
            "Epoch 45:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 45:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  85% 1560/1833 [08:51<01:33,  2.94it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  87% 1600/1833 [08:56<01:18,  2.98it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  91% 1660/1833 [09:03<00:56,  3.06it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  94% 1720/1833 [09:10<00:36,  3.13it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  97% 1780/1833 [09:17<00:16,  3.20it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Epoch 45:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.0285, v_num=0, train_loss=0.119, val_loss=4.480]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 45: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.0261, v_num=0, train_loss=0.0212, val_loss=4.550]\n",
            "Epoch 45: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.0261, v_num=0, train_loss=0.0212, val_loss=4.550]Epoch 45, global step 67435: val_loss was not in top 3\n",
            "Epoch 46:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 46:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Epoch 46:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.00791, v_num=0, train_loss=0.00107, val_loss=4.550]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 46: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00949, v_num=0, train_loss=0.00619, val_loss=4.520]\n",
            "Epoch 46: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00949, v_num=0, train_loss=0.00619, val_loss=4.520]Epoch 46, global step 68901: val_loss was not in top 3\n",
            "Epoch 47:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 47:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  85% 1560/1833 [08:51<01:33,  2.94it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  87% 1600/1833 [08:56<01:18,  2.98it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  91% 1660/1833 [09:03<00:56,  3.06it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  94% 1720/1833 [09:10<00:36,  3.13it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Epoch 47:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.00327, v_num=0, train_loss=0.00552, val_loss=4.520]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 47: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.00331, v_num=0, train_loss=0.00354, val_loss=4.490]\n",
            "Epoch 47: 100% 1833/1833 [09:24<00:00,  3.24it/s, loss=0.00331, v_num=0, train_loss=0.00354, val_loss=4.490]Epoch 47, global step 70367: val_loss was not in top 3\n",
            "Epoch 48:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 48:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  84% 1540/1833 [08:48<01:40,  2.91it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Epoch 48:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.00226, v_num=0, train_loss=0.00463, val_loss=4.490]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 48: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00237, v_num=0, train_loss=0.0015, val_loss=4.510] \n",
            "Epoch 48: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00237, v_num=0, train_loss=0.0015, val_loss=4.510]Epoch 48, global step 71833: val_loss was not in top 3\n",
            "Epoch 49:  81% 1480/1833 [08:41<02:04,  2.84it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0% 0/367 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 49:  82% 1500/1833 [08:44<01:56,  2.86it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  83% 1520/1833 [08:46<01:48,  2.89it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  84% 1540/1833 [08:49<01:40,  2.91it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  85% 1560/1833 [08:51<01:32,  2.94it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  86% 1580/1833 [08:53<01:25,  2.96it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  87% 1600/1833 [08:55<01:18,  2.99it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  88% 1620/1833 [08:58<01:10,  3.01it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  89% 1640/1833 [09:00<01:03,  3.03it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  91% 1660/1833 [09:02<00:56,  3.06it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  92% 1680/1833 [09:05<00:49,  3.08it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  93% 1700/1833 [09:07<00:42,  3.10it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  94% 1720/1833 [09:09<00:36,  3.13it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  95% 1740/1833 [09:12<00:29,  3.15it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  96% 1760/1833 [09:14<00:23,  3.17it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  97% 1780/1833 [09:16<00:16,  3.20it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  98% 1800/1833 [09:19<00:10,  3.22it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Epoch 49:  99% 1820/1833 [09:21<00:04,  3.24it/s, loss=0.00834, v_num=0, train_loss=0.00371, val_loss=4.510]\n",
            "Validating:  98% 360/367 [00:42<00:00,  8.61it/s]\u001b[A\n",
            "Epoch 49: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00743, v_num=0, train_loss=0.00148, val_loss=4.520]\n",
            "Epoch 49: 100% 1833/1833 [09:24<00:00,  3.25it/s, loss=0.00743, v_num=0, train_loss=0.00148, val_loss=4.520]Epoch 49, global step 73299: val_loss was not in top 3\n",
            "Epoch 49: 100% 1833/1833 [09:35<00:00,  3.18it/s, loss=0.00743, v_num=0, train_loss=0.00148, val_loss=4.520]\n",
            "Saving latest checkpoint...\n"
          ]
        }
      ]
    }
  ]
}